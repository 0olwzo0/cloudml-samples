{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only for when running on Colab:\n",
    "# Get the dependency .py files, if any.\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    ! git clone https://github.com/GoogleCloudPlatform/cloudml-samples.git\n",
    "    ! cp cloudml-samples/census/customestimator/trainer/* .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import model as model\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import (\n",
    "    saved_model_export_utils)\n",
    "from tensorflow.contrib.training.python.training import hparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(hparams):\n",
    "  \"\"\"Run the training and evaluate using the high level API\"\"\"\n",
    "\n",
    "  train_input = lambda: model.input_fn(\n",
    "      hparams.train_files,\n",
    "      num_epochs=hparams.num_epochs,\n",
    "      batch_size=hparams.train_batch_size\n",
    "  )\n",
    "\n",
    "  # Don't shuffle evaluation data\n",
    "  eval_input = lambda: model.input_fn(\n",
    "      hparams.eval_files,\n",
    "      batch_size=hparams.eval_batch_size,\n",
    "      shuffle=False\n",
    "  )\n",
    "\n",
    "  train_spec = tf.estimator.TrainSpec(train_input,\n",
    "                                      max_steps=hparams.train_steps\n",
    "                                      )\n",
    "\n",
    "  exporter = tf.estimator.FinalExporter('census',\n",
    "          model.SERVING_FUNCTIONS[hparams.export_format])\n",
    "  eval_spec = tf.estimator.EvalSpec(eval_input,\n",
    "                                    steps=hparams.eval_steps,\n",
    "                                    exporters=[exporter],\n",
    "                                    name='census-eval'\n",
    "                                    )\n",
    "\n",
    "  model_fn = model.generate_model_fn(\n",
    "                embedding_size=hparams.embedding_size,\n",
    "                # Construct layers sizes with exponetial decay\n",
    "                hidden_units=[\n",
    "                    max(2, int(hparams.first_layer_size *\n",
    "                               hparams.scale_factor**i))\n",
    "                    for i in range(hparams.num_layers)\n",
    "                ],\n",
    "                learning_rate=hparams.learning_rate)\n",
    "\n",
    "  estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=hparams.job_dir)\n",
    "  tf.estimator.train_and_evaluate(estimator,\n",
    "                                  train_spec,\n",
    "                                  eval_spec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# Input Arguments\n",
    "parser.add_argument(\n",
    "      '--train-files',\n",
    "      help='GCS or local paths to training data',\n",
    "      nargs='+',\n",
    "      default=['gs://cloud-samples-data/ml-engine/census/data/adult.data.csv']\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--num-epochs',\n",
    "      help=\"\"\"\\\n",
    "      Maximum number of training data epochs on which to train.\n",
    "      If both --max-steps and --num-epochs are specified,\n",
    "      the training job will run for --max-steps or --num-epochs,\n",
    "      whichever occurs first. If unspecified will run for --max-steps.\\\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--train-batch-size',\n",
    "      help='Batch size for training steps',\n",
    "      type=int,\n",
    "      default=40\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--eval-batch-size',\n",
    "      help='Batch size for evaluation steps',\n",
    "      type=int,\n",
    "      default=40\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--eval-files',\n",
    "      help='GCS or local paths to evaluation data',\n",
    "      nargs='+',\n",
    "      default=['gs://cloud-samples-data/ml-engine/census/data/adult.test.csv']\n",
    "  )\n",
    "# Training arguments\n",
    "parser.add_argument(\n",
    "      '--embedding-size',\n",
    "      help='Number of embedding dimensions for categorical columns',\n",
    "      default=8,\n",
    "      type=int\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--learning-rate',\n",
    "      help='Learning rate for the optimizer',\n",
    "      default=0.1,\n",
    "      type=float\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--first-layer-size',\n",
    "      help='Number of nodes in the first layer of the DNN',\n",
    "      default=100,\n",
    "      type=int\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--num-layers',\n",
    "      help='Number of layers in the DNN',\n",
    "      default=4,\n",
    "      type=int\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--scale-factor',\n",
    "      help='How quickly should the size of the layers in the DNN decay',\n",
    "      default=0.7,\n",
    "      type=float\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      default='/tmp/census-customestimator'\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--verbosity',\n",
    "      choices=[\n",
    "          'DEBUG',\n",
    "          'ERROR',\n",
    "          'FATAL',\n",
    "          'INFO',\n",
    "          'WARN'\n",
    "      ],\n",
    "      default='INFO',\n",
    "      help='Set logging verbosity'\n",
    "  )\n",
    "# Experiment arguments\n",
    "parser.add_argument(\n",
    "      '--train-steps',\n",
    "      help=\"\"\"\\\n",
    "      Steps to run the training job for. If --num-epochs is not specified,\n",
    "      this must be. Otherwise the training job will run indefinitely.\\\n",
    "      \"\"\",\n",
    "      default=100,\n",
    "      type=int\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--eval-steps',\n",
    "      help=\"\"\"\\\n",
    "      Number of steps to run evalution for at each checkpoint.\n",
    "      If unspecified will run until the input from --eval-files is exhausted\n",
    "      \"\"\",\n",
    "      default=None,\n",
    "      type=int\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--export-format',\n",
    "      help='The input format of the exported SavedModel binary',\n",
    "      choices=['JSON', 'CSV', 'EXAMPLE'],\n",
    "      default='JSON'\n",
    "  )\n",
    "\n",
    "  \n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "  \n",
    "# Set python level verbosity\n",
    "tf.logging.set_verbosity(args.verbosity)\n",
    "# Set C++ Graph Execution level verbosity\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(\n",
    "      tf.logging.__dict__[args.verbosity] / 10)\n",
    "\n",
    "  \n",
    "# Run the training job\n",
    "hparams=hparam.HParams(**args.__dict__)\n",
    "run_experiment(hparams)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
