{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    ! git clone https://github.com/GoogleCloudPlatform/cloudml-samples.git\n",
    "    ! cp cloudml-samples/census/estimator/trainer/* .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import model as model\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.utils import (\n",
    "    saved_model_export_utils)\n",
    "from tensorflow.contrib.training.python.training import hparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_session_config_from_env_var():\n",
    "  \"\"\"Returns a tf.ConfigProto instance that has appropriate device_filters set.\n",
    "  \"\"\"\n",
    "\n",
    "  tf_config = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
    "\n",
    "  if (tf_config and 'task' in tf_config and 'type' in tf_config['task'] and\n",
    "      'index' in tf_config['task']):\n",
    "    # Master should only communicate with itself and ps\n",
    "    if tf_config['task']['type'] == 'master':\n",
    "      return tf.ConfigProto(device_filters=['/job:ps', '/job:master'])\n",
    "    # Worker should only communicate with itself and ps\n",
    "    elif tf_config['task']['type'] == 'worker':\n",
    "      return tf.ConfigProto(device_filters=[\n",
    "          '/job:ps',\n",
    "          '/job:worker/task:%d' % tf_config['task']['index']\n",
    "      ])\n",
    "  return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(hparams):\n",
    "  \"\"\"Run the training and evaluate using the high level API\"\"\"\n",
    "\n",
    "  train_input = lambda: model.input_fn(\n",
    "      hparams.train_files,\n",
    "      num_epochs=hparams.num_epochs,\n",
    "      batch_size=hparams.train_batch_size\n",
    "  )\n",
    "\n",
    "  # Don't shuffle evaluation data\n",
    "  eval_input = lambda: model.input_fn(\n",
    "      hparams.eval_files,\n",
    "      batch_size=hparams.eval_batch_size,\n",
    "      shuffle=False\n",
    "  )\n",
    "\n",
    "  train_spec = tf.estimator.TrainSpec(train_input,\n",
    "                                      max_steps=hparams.train_steps\n",
    "                                      )\n",
    "\n",
    "  exporter = tf.estimator.FinalExporter('census',\n",
    "          model.SERVING_FUNCTIONS[hparams.export_format])\n",
    "  eval_spec = tf.estimator.EvalSpec(eval_input,\n",
    "                                    steps=hparams.eval_steps,\n",
    "                                    exporters=[exporter],\n",
    "                                    name='census-eval'\n",
    "                                    )\n",
    "\n",
    "  run_config = tf.estimator.RunConfig(session_config=_get_session_config_from_env_var())\n",
    "  run_config = run_config.replace(model_dir=hparams.job_dir)\n",
    "  print('model dir {}'.format(run_config.model_dir))\n",
    "  estimator = model.build_estimator(\n",
    "      embedding_size=hparams.embedding_size,\n",
    "      # Construct layers sizes with exponential decay\n",
    "      hidden_units=[\n",
    "          max(2, int(hparams.first_layer_size *\n",
    "                     hparams.scale_factor**i))\n",
    "          for i in range(hparams.num_layers)\n",
    "      ],\n",
    "      config=run_config\n",
    "  )\n",
    "\n",
    "  tf.estimator.train_and_evaluate(estimator,\n",
    "                                  train_spec,\n",
    "                                  eval_spec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# Input Arguments\n",
    "parser.add_argument(\n",
    "      '--train-files',\n",
    "      help='GCS or local paths to training data',\n",
    "      nargs='+',\n",
    "      default='gs://cloud-samples-data/ml-engine/census/data/adult.data.csv'\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--num-epochs',\n",
    "      help=\"\"\"\\\n",
    "      Maximum number of training data epochs on which to train.\n",
    "      If both --max-steps and --num-epochs are specified,\n",
    "      the training job will run for --max-steps or --num-epochs,\n",
    "      whichever occurs first. If unspecified will run for --max-steps.\\\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--train-batch-size',\n",
    "      help='Batch size for training steps',\n",
    "      type=int,\n",
    "      default=40\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--eval-batch-size',\n",
    "      help='Batch size for evaluation steps',\n",
    "      type=int,\n",
    "      default=40\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--eval-files',\n",
    "      help='GCS or local paths to evaluation data',\n",
    "      nargs='+',\n",
    "      default='gs://cloud-samples-data/ml-engine/census/data/adult.test.csv'\n",
    "  )\n",
    "# Training arguments\n",
    "parser.add_argument(\n",
    "      '--embedding-size',\n",
    "      help='Number of embedding dimensions for categorical columns',\n",
    "      default=8,\n",
    "      type=int\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--first-layer-size',\n",
    "      help='Number of nodes in the first layer of the DNN',\n",
    "      default=100,\n",
    "      type=int\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--num-layers',\n",
    "      help='Number of layers in the DNN',\n",
    "      default=4,\n",
    "      type=int\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--scale-factor',\n",
    "      help='How quickly should the size of the layers in the DNN decay',\n",
    "      default=0.7,\n",
    "      type=float\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      default='/tmp/census-estimator'\n",
    "  )\n",
    "\n",
    "  \n",
    "# Argument to turn on all logging\n",
    "parser.add_argument(\n",
    "      '--verbosity',\n",
    "      choices=[\n",
    "          'DEBUG',\n",
    "          'ERROR',\n",
    "          'FATAL',\n",
    "          'INFO',\n",
    "          'WARN'\n",
    "      ],\n",
    "      default='INFO',\n",
    "  )\n",
    "# Experiment arguments\n",
    "parser.add_argument(\n",
    "      '--train-steps',\n",
    "      help=\"\"\"\\\n",
    "      Steps to run the training job for. If --num-epochs is not specified,\n",
    "      this must be. Otherwise the training job will run indefinitely.\\\n",
    "      \"\"\",\n",
    "      default=100,\n",
    "      type=int\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--eval-steps',\n",
    "      help='Number of steps to run evalution for at each checkpoint',\n",
    "      default=100,\n",
    "      type=int\n",
    "  )\n",
    "parser.add_argument(\n",
    "      '--export-format',\n",
    "      help='The input format of the exported SavedModel binary',\n",
    "      choices=['JSON', 'CSV', 'EXAMPLE'],\n",
    "      default='JSON'\n",
    "  )\n",
    "\n",
    "  \n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "  \n",
    "# Set python level verbosity\n",
    "tf.logging.set_verbosity(args.verbosity)\n",
    "# Set C++ Graph Execution level verbosity\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(\n",
    "      tf.logging.__dict__[args.verbosity] / 10)\n",
    "\n",
    "  \n",
    "# Run the training job\n",
    "hparams=hparam.HParams(**args.__dict__)\n",
    "run_experiment(hparams)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
